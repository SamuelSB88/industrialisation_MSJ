"""
Refactoring du pipeline de prÃ©diction CM

Encapsule le modÃ¨le, l'encodage, le preprocessing et l'entraÃ®nement
au sein d'une classe rÃ©utilisable CMPredictor.
"""

import pandas as pd
import numpy as np
import joblib
import optuna
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBRegressor
from category_encoders import TargetEncoder, CountEncoder
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

class CMPredictor:
    def __init__(self):
        self.model = None
        self.encoder = None

    @staticmethod
    def reduce_memory_usage(df, verbose=True):
        start_mem = df.memory_usage().sum() / 1024**2
        for col in df.columns:
            col_type = df[col].dtype
            if pd.api.types.is_numeric_dtype(col_type):
                c_min = df[col].min()
                c_max = df[col].max()
                if pd.api.types.is_integer_dtype(col_type):
                    if c_min >= 0:
                        if c_max < 255:
                            df[col] = df[col].astype(np.uint8)
                        elif c_max < 65535:
                            df[col] = df[col].astype(np.uint16)
                        elif c_max < 4294967295:
                            df[col] = df[col].astype(np.uint32)
                    else:
                        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                            df[col] = df[col].astype(np.int8)
                        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                            df[col] = df[col].astype(np.int16)
                        else:
                            df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col].astype(np.float32)
        end_mem = df.memory_usage().sum() / 1024**2
        if verbose:
            print(f"ðŸ§  MÃ©moire utilisÃ©e : {start_mem:.2f} MB âžœ {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% gain)")
        return df

    @staticmethod
    def prepare_benchmark(X_train, y_train):
        Xy = X_train.copy()
        Xy['CHARGE'] = y_train['CHARGE']
        Xy['CHARGE_BIN'] = pd.qcut(Xy['CHARGE'], q=10, duplicates='drop')
        train, valid = train_test_split(Xy, test_size=0.2, stratify=Xy['CHARGE_BIN'], random_state=42)
        return train, valid

    @staticmethod
    def fill_missing(X_train, X_valid, num_cols, cat_cols):
        for df in [X_train, X_valid]:
            df[num_cols] = df[num_cols].fillna(0)
            df[cat_cols] = df[cat_cols].fillna(-999)
        return X_train, X_valid

    @staticmethod
    def compute_metrics(cm_true, cm_pred):
        rmse = np.sqrt(mean_squared_error(cm_true, cm_pred))
        rrmse = rmse / cm_true.mean()
        mape = np.mean(np.abs((cm_true - cm_pred) / cm_true)) * 100
        return rmse, rrmse, mape

    def train(self, X_train, y_train_log, X_valid, y_train_full, y_valid_full, annee_valid, cat_cols, n_trials=10):
        best_rmse = np.inf

        def objective(trial):
            nonlocal best_rmse
            encoder_type = trial.suggest_categorical("encoder", ["target", "count"])
            encoder = TargetEncoder(cols=cat_cols) if encoder_type == "target" else CountEncoder(cols=cat_cols)
            encoder.fit(X_train, y_train_log)

            X_train_enc = encoder.transform(X_train)
            X_valid_enc = encoder.transform(X_valid)

            params = {
                "n_estimators": trial.suggest_int("n_estimators", 10, 50),
                "max_depth": trial.suggest_int("max_depth", 2, 5),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
                "subsample": trial.suggest_float("subsample", 0.6, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
                "random_state": 42,
                "verbosity": 0,
                "objective": "reg:squarederror",
                "tree_method": "hist"
            }

            model = XGBRegressor(**params)
            model.fit(X_train_enc, y_train_log)

            y_pred_log = model.predict(X_valid_enc)
            y_pred = np.exp(y_pred_log)

            freq_valid = y_train_full['FREQ'].loc[X_valid.index]
            charge_valid = y_valid_full
            mask = freq_valid > 0
            cm_reel = charge_valid[mask] / (freq_valid[mask] * annee_valid[mask])
            cm_pred = y_pred[mask]

            rmse = np.sqrt(mean_squared_error(cm_reel, cm_pred))

            if rmse < best_rmse:
                best_rmse = rmse
                joblib.dump((model, encoder), "best_model_encoder_cm_xgb.pkl")
                self.model = model
                self.encoder = encoder
                print(f"\nðŸ’¾ Nouveau meilleur modÃ¨le sauvegardÃ© !\n   â†ªï¸ Encodeur : {encoder_type} | RMSE : {rmse:,.2f} â‚¬")

            return rmse

        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        return study

    def predict(self, X, numeric_columns, cat_columns):
        X[numeric_columns] = X[numeric_columns].fillna(0)
        X[cat_columns] = X[cat_columns].fillna(-999)
        X_model = X.drop(['ID', 'ANNEE_ASSURANCE'], axis=1)
        X_enc = self.encoder.transform(X_model)
        y_pred_log = self.model.predict(X_enc)
        return np.exp(y_pred_log)

    @staticmethod
    def plot_distribution(cm_true, cm_pred):
        plt.figure(figsize=(7, 3))
        sns.histplot(cm_true, bins=50, kde=True)
        plt.title("Distribution du CM rÃ©el (FREQ > 0)")
        plt.xlabel("CM rÃ©el")
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(6, 6))
        plt.scatter(cm_true, cm_pred, alpha=0.4)
        plt.xscale("log")
        plt.yscale("log")
        plt.plot([cm_true.min(), cm_true.max()], [cm_true.min(), cm_true.max()], 'r--', lw=2)
        plt.xlabel("CM rÃ©el (log)")
        plt.ylabel("CM prÃ©dit (log)")
        plt.title("ðŸ“Š CM prÃ©dit vs rÃ©el (log-log, FREQ > 0)")
        plt.grid(True, which="both", ls="--")
        plt.tight_layout()
        plt.show()
